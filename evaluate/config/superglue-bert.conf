// Config settings used for SuperGLUE BERT baseline experiments.

// This imports the defaults, which can be overridden below.
include "defaults.conf"
// exp_name = "bert-large-cased"

// Data and preprocessing settings
max_seq_len = 256 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.
// tokenizer = "bert-large-cased"
tokenizer = "bert-base-uncased"
// Model settings
// input_module = "bert-large-cased"
input_module = "bert-base-uncased"
run_name_suffix = ""
noise_ratio = 0
accumulate_grad_iter = 1
bert_embeddings_mode = "top"
// pool_type = "first"
pool_type = "first_init"
pooler_dropout = 0
pooler_scalar_dropout = 0
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
few_shot = -1
random_file_path = finetuned_berts/random_numbers.pt
// few_shot = 1000
unnorm_facet = 1
// unnorm_facet = 0
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model
data_dir="superglue_data"

// Training settings
dropout = 0.1 // following BERT paper
optimizer = adam
batch_size = 4
max_epochs = 20
lr = .00001
// min_lr = .0000001
min_lr = 0
lr_patience = 4
patience = 20
max_vals = 10000
warmup_ratio = 0.1
weight_decay = 0

reload_vocab = 1
// reload_tasks = 1
// reload_indexing = 1
// reindex_tasks = "boolq,commitbank,copa,multirc,record,rte-superglue,winograd-coreference,wic,broadcoverage-diagnostic,winogender-diagnostic"

// Tasks
pretrain_tasks = superglue
target_tasks = superglue

// Control-flow stuff
// do_pretrain = 1
do_pretrain = 0
do_target_task_training = 1
// do_full_eval = 1
do_full_eval = 0
// write_preds = "val,test"
write_preds = ""
write_strict_glue_format = 1
analyze_grad = 0
